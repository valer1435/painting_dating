{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler \n",
    "from torchvision import transforms\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_IMAGES_PATH = os.sep.join([\"..\", \"resized_images\"]) # относительный путь к папке с картинками\n",
    "\n",
    "CSV_PATH = \"sample_full.csv\"\n",
    "\n",
    "IMAGE_NORMAL_SIZE = (224, 224)\n",
    "GRAM_SIZE = 512\n",
    "\n",
    "RANDOM_SEED = 4545435\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "CPU = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCustomDataset(Dataset):\n",
    "    def __init__(self, dict_path):\n",
    "        self.transformations = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "        self.class_dict = pd.read_csv(dict_path)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        row = self.class_dict.iloc[index]\n",
    "        file_path = row[1]\n",
    "        \n",
    "        data = Image.open(file_path).convert(\"RGB\").resize(IMAGE_NORMAL_SIZE)\n",
    "        data = self.transformations(data)  # (3)\n",
    "        \n",
    "        label = row[2]\n",
    "        \n",
    "        return data, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.class_dict.index.shape[0]\n",
    "    \n",
    "\n",
    "class StyleMatrix(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StyleMatrix, self).__init__()\n",
    "\n",
    "    def forward(self, input_):\n",
    "         return __class__.gram_matrix(input_)\n",
    "    \n",
    "    @staticmethod\n",
    "    def gram_matrix(inp): # исправленная версия\n",
    "        a, b, c, d = inp.size()  # a=batch size(=1)\n",
    "        # b=number of feature maps\n",
    "        # (c, d)=dimensions of a f. map (N=c*d)\n",
    "\n",
    "        features = inp.view(a, b, c * d)  # resise F_XL into \\hat F_XL\n",
    "        G = torch.empty((a, b, b))\n",
    "        for i in range(a):\n",
    "            G[i] = torch.mm(features[i], features[i].t())\n",
    "\n",
    "        # we 'normalize' the values of the gram matrix\n",
    "        # by dividing by the number of element in each feature maps.\n",
    "        return G.div(a * b * c * d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Урезанная сеть ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\Valery/.cache\\torch\\hub\\pytorch_vision_v0.6.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (2): ReLU(inplace=True)\n",
       "  (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (5): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (6): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (7): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (8): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (9): StyleMatrix()\n",
       ")"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet_18 = torch.hub.load('pytorch/vision:v0.6.0', 'resnet18', pretrained=True)\n",
    "# or any of these variants\n",
    "# model = torch.hub.load('pytorch/vision:v0.6.0', 'resnet34', pretrained=True)\n",
    "# model = torch.hub.load('pytorch/vision:v0.6.0', 'resnet50', pretrained=True)\n",
    "# model = torch.hub.load('pytorch/vision:v0.6.0', 'resnet101', pretrained=True)\n",
    "# model = torch.hub.load('pytorch/vision:v0.6.0', 'resnet152', pretrained=True)\n",
    "our_resnet_18 = torch.nn.Sequential(*list(resnet_18.children())[:-1]+[StyleMatrix()])\n",
    "torch.save(our_resnet_18.state_dict(), \"our_resnet_18.pth\") # на случай, если придется пересохранять сетку\n",
    "our_resnet_18.load_state_dict(torch.load(\"our_resnet_18.pth\"))\n",
    "our_resnet_18.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создание списка всех изображений (номер - путь к файлу - класс) \n",
    "Аргументы: file_path-название результирующего файла, cut_factor-как \"порезать\" датасет (cut_factor=2 будет выбирать только каждую 2 картинку.  \n",
    "Чем больше cut_factor-тем меньше датасет, выбрать полный датасет-cut_factor=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_csv_classes(file_path, cut_factor=1):\n",
    "    header = \"filename, full_filename, num_class\"\n",
    "    data = header + \"\\n\"\n",
    "    \n",
    "    timeframes = os.listdir(TRAIN_IMAGES_PATH)\n",
    "    \n",
    "    with open(file_path, \"w+\") as file_csv:\n",
    "        for count_class, timeframe in enumerate(timeframes):\n",
    "            timeframe_images_path = TRAIN_IMAGES_PATH + os.sep + timeframe\n",
    "            images = os.listdir(timeframe_images_path)\n",
    "            img_count = 0\n",
    "            for image_name in images:\n",
    "                img_count+=1\n",
    "                if img_count % cut_factor == 0: \n",
    "                    image_index = image_name.split(\".\")[0]\n",
    "                    image_path = timeframe_images_path + os.sep + image_name\n",
    "                    data += \"{},{},{}\\n\".format(image_index, image_path, count_class)\n",
    "                \n",
    "        file_csv.write(data)\n",
    "CUT_FACTOR = 32\n",
    "generate_csv_classes(CSV_PATH, cut_factor=CUT_FACTOR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузчики данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALIDATION_SPLIT = 0.4\n",
    "TEST_SPLIT = 0.5\n",
    "BATCH_SIZE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches in train loader: 580 (580 images)\n",
      "Number of batches in validation loader: 386 (386 images)\n"
     ]
    }
   ],
   "source": [
    "data = MyCustomDataset(CSV_PATH)\n",
    "\n",
    "dataset_size = len(data) \n",
    "indices = list(range(dataset_size)) \n",
    "\n",
    "split = int(VALIDATION_SPLIT * dataset_size)\n",
    "\n",
    "np.random.seed(RANDOM_SEED) \n",
    "np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices) \n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = DataLoader(data, batch_size=BATCH_SIZE, num_workers=0, sampler=train_sampler)\n",
    "\n",
    "val_loader = DataLoader(data, batch_size = BATCH_SIZE, num_workers=0, sampler=valid_sampler)\n",
    "\n",
    "print(\"Number of batches in train loader: {} ({} images)\".format(len(train_loader), len(train_loader) * BATCH_SIZE))\n",
    "print(\"Number of batches in validation loader: {} ({} images)\".format(len(val_loader), len(val_loader) * BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_MASK_PATH = \"features8000.npy\" # путь к файлу с маской признаков (в данном случае на 8192 штуки)\n",
    "TRAIN_CSV_PATH = \"train.csv\"\n",
    "VALIDATION_CSV_PATH = \"val.csv\"\n",
    "TEST_CSV_PATH = \"test.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Отбор признаков (можно пропустить)\n",
    "Каждую итерацию исключается ровно половина признаков, выявление наиболее важных происходит на 500, 1000, ... объектах в зависимости от текущего количества неисключенных признаков. В конце остаётся ~4000, которые можно сохранить в csv или продолжить работу.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "def select_features(mask, number, k):\n",
    "#     inds = np.random.choice(len(data), size=min(len(data), number), replace=False)\n",
    "    X = []\n",
    "    y = []\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        G_matrix = our_resnet_18(torch.reshape(images, (1, 3, 224, 224)).to(DEVICE))\n",
    "        X.append((G_matrix.to(CPU).detach().numpy().reshape((1,512*512))[0][mask] + 1) / 2)\n",
    "        y.append(labels)\n",
    "        if i > min(len(data), number):\n",
    "            break\n",
    "    selector = SelectKBest(chi2, k=k)\n",
    "\n",
    "    selector.fit(X, y)\n",
    "    return selector.get_support(indices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 objects\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\valery\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:8: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_features = 131072\n",
      "39.731868743896484 seconds spent\n",
      "1000 objects\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\valery\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:8: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_features = 65536\n",
      "82.13765978813171 seconds spent\n",
      "2000 objects\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\valery\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:8: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_features = 32768\n",
      "124.334712266922 seconds spent\n",
      "4000 objects\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\valery\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:8: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_features = 16384\n",
      "169.82481837272644 seconds spent\n",
      "8000 objects\n",
      "n_features = 8192\n",
      "218.26464819908142 seconds spent\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\valery\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:8: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "mask_history = []\n",
    "\n",
    "mask = np.array([i for i in range(262144)])\n",
    "while len(mask) > 10000:\n",
    "    print(500 * 262144 // len(mask), 'objects')\n",
    "    mask = mask[[list(select_features(mask, 500 * 262144 // len(mask), len(mask) // 2))]]\n",
    "    mask_history.append(mask)\n",
    "    print('n_features =', len(mask))\n",
    "    print((time.time() - start_time), 'seconds spent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сохраним номера признаков на разных шагах в файл на случай их утери."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import asarray, save\n",
    "\n",
    "save('features_resnet_65000.npy', asarray(mask_history[-4]))\n",
    "save('features_resnet_32000.npy', asarray(mask_history[-3]))\n",
    "save('features_resnet_16000.npy', asarray(mask_history[-2]))\n",
    "save('features_resnet_8000.npy', asarray(mask_history[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Перегонка картинок в их признаковое описание\n",
    "Выполнение этого блока необходимо, если у вас нет сконвертированных данных в .csv\n",
    "\n",
    "Если таковые есть, то этот блок стоит пропустить"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of selected features: 8192\n"
     ]
    }
   ],
   "source": [
    "mask = np.load(FEATURES_MASK_PATH)\n",
    "print(\"Number of selected features:\", len(mask))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 batches out of 580 completed\n",
      "Done. Converting results to Pandas DataFrame...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "def get_features_dataset(loader, features, model):\n",
    "    X = []\n",
    "    y = []\n",
    "    size = len(loader)\n",
    "    for i, (images, labels) in enumerate(loader):\n",
    "        actual_batch_size = min(images.shape[0], BATCH_SIZE)\n",
    "        G_matrices = model(torch.reshape(images, (actual_batch_size, 3, *IMAGE_NORMAL_SIZE)).to(DEVICE))\n",
    "\n",
    "        for j in range(actual_batch_size):\n",
    "            X.append(G_matrices[j].cpu().detach().numpy().reshape((1, GRAM_SIZE*GRAM_SIZE))[0][features])\n",
    "        y += labels.tolist()\n",
    "        if (i % 1000) == 0:\n",
    "            print(i, \"batches out of\", size, \"completed\")\n",
    "    print(\"Done. Converting results to Pandas DataFrame...\")\n",
    "    df = pd.DataFrame(data=X)\n",
    "    df[\"label\"] = y\n",
    "    print(\"Done.\")\n",
    "    return df\n",
    "\n",
    "df_train = get_features_dataset(train_loader, mask, our_resnet_18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 batches out of 386 completed\n",
      "Done. Converting results to Pandas DataFrame...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "df_val = get_features_dataset(val_loader, mask, our_resnet_18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = int(df_val.shape[0] * TEST_SPLIT)\n",
    "df_test, df_val = df_val.iloc[:test_size, :], df_val.iloc[test_size:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((580, 8193), (193, 8193), (193, 8193))"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape, df_val.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved train data\n",
      "Saved validation data\n",
      "Saved test data\n"
     ]
    }
   ],
   "source": [
    "# можно пропустить, если сохранение не требуется\n",
    "df_train.to_csv(TRAIN_CSV_PATH)\n",
    "print(\"Saved train data\")\n",
    "\n",
    "df_val.to_csv(VALIDATION_CSV_PATH)\n",
    "print(\"Saved validation data\")\n",
    "\n",
    "df_test.to_csv(TEST_CSV_PATH)\n",
    "print(\"Saved test data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка данных из CSV файлов\n",
    "Этот блок необходимо выполнить, если был пропущен предыдущий"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded train data\n",
      "Loaded validation data\n",
      "Loaded test data\n"
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(TRAIN_CSV_PATH)\n",
    "print(\"Loaded train data\")\n",
    "\n",
    "df_val = pd.read_csv(VALIDATION_CSV_PATH)\n",
    "print(\"Loaded validation data\")\n",
    "\n",
    "df_test = pd.read_csv(TEST_CSV_PATH)\n",
    "print(\"Loaded test data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение и тестирование моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import f1_score, mean_squared_error, mean_squared_log_error, confusion_matrix, plot_confusion_matrix\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train.loc[:, df_train.columns != \"label\"]\n",
    "X_test = df_test.loc[:, df_test.columns != \"label\"]\n",
    "X_val = df_val.loc[:, df_val.columns != \"label\"]\n",
    "\n",
    "y_train = df_train[\"label\"]\n",
    "y_test = df_test[\"label\"]\n",
    "y_val = df_val[\"label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\valery\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "def replace_1300(y): # объединение малочисленных классов\n",
    "    for i in y.index:\n",
    "        y[i] = max(y[i], 16)\n",
    "        \n",
    "replace_1300(y_train)\n",
    "replace_1300(y_test)\n",
    "replace_1300(y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Используем SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(n_jobs=-1)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd = SGDClassifier(n_jobs=-1, verbose=0)\n",
    "sgd.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06735751295336788\n",
      "16.383419689119172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\valery\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:8: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sgd_predict = sgd.predict(X_val)\n",
    "\n",
    "print(f1_score(y_val, sgd_predict, average=\"micro\"), mean_squared_error(y_val, sgd_predict), sep=\"\\n\")\n",
    "\n",
    "cmat = confusion_matrix(y_val, sgd_predict)\n",
    "plt.figure(figsize=(7, 7))\n",
    "sns.heatmap(\n",
    "    cmat / np.sum(cmat, axis=0),\n",
    "    annot=False, cbar=False,\n",
    "    xticklabels=[1300 + i*50 for i in range(13)],\n",
    "    yticklabels=[1300 + i*50 for i in range(13)],\n",
    "    cmap=\"Blues\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Используем SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\valery\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\svm\\_base.py:977: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVC(verbose=2)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_svc = LinearSVC(verbose=2, max_iter=1000)\n",
    "linear_svc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.13471502590673576\n",
      "16.10362694300518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\valery\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:8: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "svc_predict = linear_svc.predict(X_test)\n",
    "\n",
    "print(f1_score(y_test, svc_predict, average=\"micro\"), mean_squared_error(y_test, svc_predict), sep=\"\\n\")\n",
    "\n",
    "cmat = confusion_matrix(y_test, svc_predict)\n",
    "plt.figure(figsize=(6,6))\n",
    "sns.heatmap(\n",
    "    cmat / np.sum(cmat, axis=0),\n",
    "    annot=False, cbar=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
